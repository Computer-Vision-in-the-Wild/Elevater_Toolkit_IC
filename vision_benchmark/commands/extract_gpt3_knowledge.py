"""
Zero shot evaluation.
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import argparse
import logging

import time
import numpy as np

from vision_benchmark.common.utils import log_arg_env_config, submit_predictions
from vision_benchmark.utils import comm, create_logger
from vision_benchmark.common.constants import get_dataset_hub, VISION_DATASET_STORAGE
from vision_benchmark.datasets import SimpleTokenizer, HFPTTokenizer, class_map, template_map
from vision_benchmark.evaluation import extract_features, extract_text_features, clip_zeroshot_evaluator
from vision_benchmark.config import config, update_config

import random
from tqdm import tqdm
import openai

import csv
import pdb

ds_list=['eurosat-clip','country211','kitti-distance','oxford-iiit-pets','ping-attack-on-titan-plus','ping-whiskey-plus','rendered-sst2','resisc45-clip','voc2007classification','caltech101','cifar10','cifar100','dtd','fer2013','fgvc-aircraft-2013b','flower102','food101','gtsrb','hateful-memes','mnist','patchcamelyon','stanfordcar']

def add_zero_shot_args(parser):
    parser.add_argument('--ds', required=False, help='Evaluation dataset configure file name.', type=str)

    parser.add_argument('--target', help='target of run. local or azureml', choices=['local', 'azureml'], default='local')
    parser.add_argument('--wiki_knowledge_path', help='the path to wikitionary.', default='../vision_benchmark/resources/knowledge/external', type=str)


    # GPT-3 
    parser.add_argument('--gpt3', help='extract gpt3 knowledge.', default=False, action='store_true')
    parser.add_argument('--max_num_questions', default="5", type=int, help='Max number of question to ask GPT-3. note that -1 indicate the entire dataset')
    parser.add_argument('--apikey', type=str, required=True, help='api key; openaiapi@microsoft.com')
    parser.add_argument('--max_tokens', type=int, default=50, help="number of max_tokens to step in the decoding stage")
    parser.add_argument('--num_dpr_contexts', type=int, default=1, help="the nubmer of dpr retrieved context to use")
    parser.add_argument('--n_shot', type=int, default=1, help="number of shots used in in-context-learning")
    parser.add_argument('--n_ensemble', type=int, default=1, help="number of knowledge items generated by GPT3")

    parser.add_argument('opts',
                        help="Modify config options using the command-line",
                        default=None,
                        nargs=argparse.REMAINDER)    

def _construct_command(args):
    """Build a commandline from the parsed arguments"""

    cmd = ['vb_zero_shot_eval', '--ds', args.ds, '--model', args.model, '--target', args.target]

    if args.submit_predictions:
        assert args.submit_by
        cmd.extend(['--submit-predictions', '--submit-by', args.submit_by])

    return cmd


## guarantee to get prediction
def random_query(name_test, name2wiki_exist, args):

    for ni in range(args.n_shot):
        train_item_selected = name2wiki_exist[random.randint(0,len(name2wiki_exist)-1)]
        prompt += 'Q: %s\nA: %s\n\n===\n'%( train_item_selected[0], train_item_selected[1] )
    prompt += 'Q: %s\nA:'%name_test
    
    error_count,response = 0, None

    while True:
        try:
            response = openai.Completion.create(
              engine="text-davinci-002",  # "davinci-msft",
              prompt=prompt,
              max_tokens=5,
              logprobs=1,
              temperature=0.,
              stream=False,
              stop=["\n", "<|endoftext|>"]
            )
        except:
            time.sleep(60)
            continue
        assert(response is not None)
        return response

def ask_gpt3(args, name_test, name2wiki_exist):

    repeat_count = args.n_ensemble
    n_shot = min(args.n_shot, len(name2wiki_exist))
    
    pred_answer_list, pred_prob_list = [], []
    context_key_list = None
    # context_key_list = get_context_keys(key, metric=args.sample_select,n=n_shot*repeat_count,valid_keys=traincontext_question_dict.keys())

    for repeat in range(repeat_count):
        prompt = 'Please explain the concept according to the context.\n===\n'

        for ni in range(n_shot):
            train_item_selected = name2wiki_exist[random.randint(0,len(name2wiki_exist)-1)]
            prompt += 'Q: %s\nA: %s\n\n===\n'%( train_item_selected[0], train_item_selected[1] )
        prompt += 'Q: %s\nA:'%name_test

        error_count, response = 0, None

        # print(f'Prompt: {prompt}')
        while True:
            try:
                response = openai.Completion.create(
                  engine="text-davinci-002",  # "davinci-msft", switch to other variants if necessary
                  prompt=prompt,
                  max_tokens=args.max_tokens,
                  logprobs=1,
                  temperature=0.,
                  stream=False,
                  stop=["\n", "<|endoftext|>"]
                )
            except Exception as e:
                ## if overlength, use half in-context examples to get the results
                if 'maximum context length is' in str(e):
                    response = random_query(name_test, name2wiki_exist, args)
                    break
                ## system could overload, sleep and re-try
                time.sleep(60)
                if error_count>3:
                    print(e)
                    break
                error_count += 1
                continue
            break
        if response is None:
            response = random_query(name_test, name2wiki_exist, args)
            
        plist = []
        for ii in range(len(response['choices'][0]['logprobs']['tokens'])):
            if response['choices'][0]['logprobs']['tokens'][ii]=='\n':
                break
            plist.append(response['choices'][0]['logprobs']['token_logprobs'][ii])
        pred_answer_list.append( response['choices'][0]["text"] )
        pred_prob_list.append(sum(plist))

    # print(f'GPT-3: {pred_answer_list} \n\n')

    return pred_answer_list, pred_prob_list

def extract_gpt3_konwledge(config, args):

    import sys, json
    sys.path.append(args.wiki_knowledge_path)

    class_names = class_map.get(config.DATASET.DATASET)
    if not class_names:
        hub = get_dataset_hub()
        from vision_datasets import Usages
        manifest = hub.create_dataset_manifest(VISION_DATASET_STORAGE, None, config.DATASET.DATASET, usage=Usages.TEST_PURPOSE)
        if manifest:
            class_names = manifest[0].labelmap

    
    
    
    print(class_names)
    wiki_tsv_path = os.path.join(args.wiki_knowledge_path,  config.DATASET.DATASET + '_knowledge.tsv') 
    wiki_anwser_list = json.load(open(wiki_tsv_path, encoding='utf-8'))    



    count_has_wiki_knowledge = 0
    wiki_dict = {}
    name2wiki_exist = []
    for k2v in wiki_anwser_list:
        wiki_dict[ k2v['classname'] ] = k2v['def_wiki']   
        if k2v['def_wiki']:
            count_has_wiki_knowledge += 1
            name2wiki_exist.append( (k2v['classname'], k2v['def_wiki']) )
    print(f'The wiki knowledge coverage is {count_has_wiki_knowledge} / {len(wiki_dict)}')


    # collect knowledge from gpt3, based on wiki

    start = time.time()
    if True:
        openai.api_key = args.apikey

        gpt3_answer_list = []
        for name_query in wiki_anwser_list:
            name_query = k2v['classname']
            pred_answer_list, pred_prob_list = ask_gpt3(args, name_query, name2wiki_exist)
            print(f'GPT3 | {name_query}: {pred_answer_list}')
                    
            gpt3_answer_dict = {}
            gpt3_answer_dict['classname'] = name_query
            gpt3_answer_dict['gpt3'] = pred_answer_list
            gpt3_answer_list.append(gpt3_answer_dict)

        
        logging.info(f'=> GPT3 knowledge extraction duration time: {time.time() - start:.2f}')

        gpt3_results_json_filename = 'GPT3_' + config.DATASET.DATASET + '.tsv'
        with open( os.path.join(config.OUTPUT_DIR, gpt3_results_json_filename) , 'w') as json_file:
            # print(f"gpt3_answer_dict {gpt3_answer_dict}")
            json.dump(gpt3_answer_list, json_file)
        json_file.close()

    return class_names




def main():

    parser = argparse.ArgumentParser(description='Extract GPT3 Knolwedge script.')
    add_zero_shot_args(parser)
    args = parser.parse_args()

    args.cfg = args.ds
    update_config(config, args)
    config.defrost()
    config.NAME = ""
    config.freeze()


    if args.target == 'azureml':
        from vision_benchmark.common.run_aml import run_aml
        setattr(args, 'target', 'local')
        run_aml(args, _construct_command, 'zero_shot')
        return

    exp_name = 'zeroshot_eval_' + f'wiki_{config.KNOWLEDGE.WIKITIONARY.USE_DEFINITION}_wnh_{config.KNOWLEDGE.WORDNET.USE_HIERARCHY}_wnd_{config.KNOWLEDGE.WORDNET.USE_DEFINITION}_gpt3_{config.KNOWLEDGE.GPT3.USE_GPT3}'

    final_output_dir = create_logger(config, exp_name)
    
    if comm.is_main_process():
        log_arg_env_config(args, config, final_output_dir)


    extract_gpt3_konwledge(config, args)


if __name__ == '__main__':
    main()
